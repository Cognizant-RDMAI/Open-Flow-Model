{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec2ee1f3-cda1-4eaa-9227-3877aa6d4569",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda5dd07",
   "metadata": {},
   "source": [
    "## I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b77988fd-61bc-483e-92ec-4ac9d2ca9505",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "from util_IO import (\n",
    "    load_pickle_from_main_project_dir,\n",
    "    load_attributes_df,\n",
    "    load_timeseries_df\n",
    ")\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f039eaf",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "572a8620",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 13:12:47.298824: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-09 13:12:48.257109: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-05-09 13:12:48.257217: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu/:/opt/conda/lib\n",
      "2025-05-09 13:12:48.257227: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import timeseries_dataset_from_array\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e97b27-f085-449a-b973-67e6044cd2d5",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00ea940-9131-443c-8b77-05fc0a113c3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72258a0d-26c1-4063-afdd-202d9dd2c186",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set pandas to display a maximum of 300 columns\n",
    "pd.set_option('display.max_columns', 300)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "# Suppress the SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4079b01-236e-40ae-8e78-0ab23170f112",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b809059-edbc-4bfc-820a-78d6b2471172",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load metadata from *1-DataAggregation.ipynb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a470830f-e526-4ce3-ae97-cb9651b96659",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attributes': {'aggregations': {'fundamental': {'chalk_streams_df': ['chalk_stream_flag'],\n",
      "                                                 'climatic_attributes_df': [],\n",
      "                                                 'humaninfluence_attributes_df': ['surfacewater_abs',\n",
      "                                                                                  'groundwater_abs',\n",
      "                                                                                  'discharges',\n",
      "                                                                                  'num_reservoir',\n",
      "                                                                                  'reservoir_cap'],\n",
      "                                                 'hydrogeology_attributes_df': [],\n",
      "                                                 'hydrologic_attributes_df': ['baseflow_index'],\n",
      "                                                 'hydrometry_attributes_df': ['bankfull_flow'],\n",
      "                                                 'landcover_attributes_df': ['dwood_perc',\n",
      "                                                                             'ewood_perc',\n",
      "                                                                             'grass_perc',\n",
      "                                                                             'shrub_perc',\n",
      "                                                                             'crop_perc',\n",
      "                                                                             'urban_perc',\n",
      "                                                                             'inwater_perc',\n",
      "                                                                             'bares_perc'],\n",
      "                                                 'soil_attributes_df': ['sand_perc',\n",
      "                                                                        'silt_perc',\n",
      "                                                                        'clay_perc',\n",
      "                                                                        'organic_perc'],\n",
      "                                                 'topographic_attributes_df': ['gauge_name',\n",
      "                                                                               'gauge_lat',\n",
      "                                                                               'gauge_lon',\n",
      "                                                                               'gauge_elev',\n",
      "                                                                               'area',\n",
      "                                                                               'dpsbar',\n",
      "                                                                               'elev_mean',\n",
      "                                                                               'elev_min',\n",
      "                                                                               'elev_10',\n",
      "                                                                               'elev_50',\n",
      "                                                                               'elev_90',\n",
      "                                                                               'elev_max']}},\n",
      "                'attributes_files_ext': '.csv',\n",
      "                'attributes_index': 'gauge_id'},\n",
      " 'camels_gb_bronze_dir': '/home/jupyter/RDMAI_Sensing/flow-derivation-camels-gb/datasets/camels-gb/data',\n",
      " 'camels_gb_data_attributes_aggr_dir': '/home/jupyter/RDMAI_Sensing/flow-derivation-camels-gb/datasets/camels-gb-aggregated/attributes',\n",
      " 'camels_gb_data_attributes_dir': '/home/jupyter/RDMAI_Sensing/flow-derivation-camels-gb/datasets/camels-gb/data',\n",
      " 'camels_gb_data_timeseries_aggr_dir': '/home/jupyter/RDMAI_Sensing/flow-derivation-camels-gb/datasets/camels-gb-aggregated/timeseries',\n",
      " 'camels_gb_data_timeseries_dir': '/home/jupyter/RDMAI_Sensing/flow-derivation-camels-gb/datasets/camels-gb/data/timeseries',\n",
      " 'camels_gb_datasets_dir': '/home/jupyter/RDMAI_Sensing/flow-derivation-camels-gb/datasets',\n",
      " 'camels_gb_silver_dir': '/home/jupyter/RDMAI_Sensing/flow-derivation-camels-gb/datasets/camels-gb-aggregated',\n",
      " 'camels_gb_use_case_dir': '/home/jupyter/RDMAI_Sensing/flow-derivation-camels-gb',\n",
      " 'timeseries': {'catchmentID_pattern': 'CAMELS_GB_hydromet_timeseries_(.*?)_19701001-20150930',\n",
      "                'date_field': 'date',\n",
      "                'label_field': 'discharge_vol',\n",
      "                'timeseries_aggr_issues_dict': {},\n",
      "                'timeseries_files_ext': '.csv'}}\n"
     ]
    }
   ],
   "source": [
    "aggr_parameters_dict, camels_gb_use_case_dir = load_pickle_from_main_project_dir(\n",
    "    'aggr_parameters_dict.pkl'\n",
    ")\n",
    "\n",
    "# # Print imported variable\n",
    "pprint(aggr_parameters_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d12caeb5-7fd3-4dc3-8796-6a590e55dbb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Variables picked\n",
    "attributes_index = aggr_parameters_dict[\"attributes\"][\"attributes_index\"]\n",
    "date_field = aggr_parameters_dict['timeseries']['date_field']\n",
    "label_field = aggr_parameters_dict['timeseries']['label_field']\n",
    "camels_gb_silver_dir = aggr_parameters_dict['camels_gb_silver_dir']\n",
    "camels_gb_data_attributes_aggr_dir = aggr_parameters_dict['camels_gb_data_attributes_aggr_dir']\n",
    "camels_gb_data_timeseries_aggr_dir = aggr_parameters_dict['camels_gb_data_timeseries_aggr_dir']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55133c9-82e4-4d4d-a071-dba7ab34fc8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define reference data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bcbc5ee-9ffb-4748-b619-0afcd3d335aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting \n",
    "example_input = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0970b55-2002-4dfe-b375-baf1bc1c30bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>catchmentID</th>\n",
       "      <th>date</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>shortwave_rad</th>\n",
       "      <th>longwave_rad</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>discharge_vol</th>\n",
       "      <th>date_group</th>\n",
       "      <th>log1p_discharge_vol</th>\n",
       "      <th>sin_year</th>\n",
       "      <th>cos_year</th>\n",
       "      <th>time_ref</th>\n",
       "      <th>time_ref_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101002</td>\n",
       "      <td>1997-03-01</td>\n",
       "      <td>0.26</td>\n",
       "      <td>8.31</td>\n",
       "      <td>6.11</td>\n",
       "      <td>73.68</td>\n",
       "      <td>320.09</td>\n",
       "      <td>6.23</td>\n",
       "      <td>0.319</td>\n",
       "      <td>00</td>\n",
       "      <td>0.276874</td>\n",
       "      <td>-0.326029</td>\n",
       "      <td>0.945360</td>\n",
       "      <td>9648</td>\n",
       "      <td>0.587040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101002</td>\n",
       "      <td>1997-03-02</td>\n",
       "      <td>0.10</td>\n",
       "      <td>9.55</td>\n",
       "      <td>5.64</td>\n",
       "      <td>89.11</td>\n",
       "      <td>315.49</td>\n",
       "      <td>6.51</td>\n",
       "      <td>0.314</td>\n",
       "      <td>00</td>\n",
       "      <td>0.273076</td>\n",
       "      <td>-0.309718</td>\n",
       "      <td>0.950828</td>\n",
       "      <td>9649</td>\n",
       "      <td>0.587101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101002</td>\n",
       "      <td>1997-03-03</td>\n",
       "      <td>24.15</td>\n",
       "      <td>5.84</td>\n",
       "      <td>4.98</td>\n",
       "      <td>51.62</td>\n",
       "      <td>320.09</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.397</td>\n",
       "      <td>00</td>\n",
       "      <td>0.334327</td>\n",
       "      <td>-0.293316</td>\n",
       "      <td>0.956015</td>\n",
       "      <td>9650</td>\n",
       "      <td>0.587162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  catchmentID        date  precipitation  temperature  humidity  \\\n",
       "0      101002  1997-03-01           0.26         8.31      6.11   \n",
       "1      101002  1997-03-02           0.10         9.55      5.64   \n",
       "2      101002  1997-03-03          24.15         5.84      4.98   \n",
       "\n",
       "   shortwave_rad  longwave_rad  windspeed  discharge_vol date_group  \\\n",
       "0          73.68        320.09       6.23          0.319         00   \n",
       "1          89.11        315.49       6.51          0.314         00   \n",
       "2          51.62        320.09       2.75          0.397         00   \n",
       "\n",
       "   log1p_discharge_vol  sin_year  cos_year  time_ref  time_ref_scaled  \n",
       "0             0.276874 -0.326029  0.945360      9648         0.587040  \n",
       "1             0.273076 -0.309718  0.950828      9649         0.587101  \n",
       "2             0.334327 -0.293316  0.956015      9650         0.587162  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseflow_index</th>\n",
       "      <th>sand_perc</th>\n",
       "      <th>silt_perc</th>\n",
       "      <th>clay_perc</th>\n",
       "      <th>organic_perc</th>\n",
       "      <th>gauge_elev</th>\n",
       "      <th>area</th>\n",
       "      <th>dpsbar</th>\n",
       "      <th>elev_mean</th>\n",
       "      <th>elev_min</th>\n",
       "      <th>elev_10</th>\n",
       "      <th>elev_50</th>\n",
       "      <th>elev_90</th>\n",
       "      <th>elev_max</th>\n",
       "      <th>dwood_perc</th>\n",
       "      <th>ewood_perc</th>\n",
       "      <th>grass_perc</th>\n",
       "      <th>shrub_perc</th>\n",
       "      <th>crop_perc</th>\n",
       "      <th>urban_perc</th>\n",
       "      <th>inwater_perc</th>\n",
       "      <th>bares_perc</th>\n",
       "      <th>surfacewater_abs</th>\n",
       "      <th>groundwater_abs</th>\n",
       "      <th>discharges</th>\n",
       "      <th>num_reservoir</th>\n",
       "      <th>reservoir_cap</th>\n",
       "      <th>chalk_stream_flag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gauge_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53018</th>\n",
       "      <td>0.57</td>\n",
       "      <td>31.51</td>\n",
       "      <td>32.07</td>\n",
       "      <td>36.42</td>\n",
       "      <td>0.74</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1569.35</td>\n",
       "      <td>48.9</td>\n",
       "      <td>109.0</td>\n",
       "      <td>20.3</td>\n",
       "      <td>53.3</td>\n",
       "      <td>104.0</td>\n",
       "      <td>172.1</td>\n",
       "      <td>304.6</td>\n",
       "      <td>6.38</td>\n",
       "      <td>0.65</td>\n",
       "      <td>50.46</td>\n",
       "      <td>0.04</td>\n",
       "      <td>35.01</td>\n",
       "      <td>6.91</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33034</th>\n",
       "      <td>0.77</td>\n",
       "      <td>56.74</td>\n",
       "      <td>23.23</td>\n",
       "      <td>20.03</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.2</td>\n",
       "      <td>707.75</td>\n",
       "      <td>16.3</td>\n",
       "      <td>42.0</td>\n",
       "      <td>8.1</td>\n",
       "      <td>25.3</td>\n",
       "      <td>41.7</td>\n",
       "      <td>60.0</td>\n",
       "      <td>94.6</td>\n",
       "      <td>8.49</td>\n",
       "      <td>7.07</td>\n",
       "      <td>13.07</td>\n",
       "      <td>0.33</td>\n",
       "      <td>65.76</td>\n",
       "      <td>5.14</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55018</th>\n",
       "      <td>0.56</td>\n",
       "      <td>36.89</td>\n",
       "      <td>36.01</td>\n",
       "      <td>27.09</td>\n",
       "      <td>0.47</td>\n",
       "      <td>55.4</td>\n",
       "      <td>143.82</td>\n",
       "      <td>68.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>57.2</td>\n",
       "      <td>77.0</td>\n",
       "      <td>142.3</td>\n",
       "      <td>199.2</td>\n",
       "      <td>252.4</td>\n",
       "      <td>5.40</td>\n",
       "      <td>0.12</td>\n",
       "      <td>56.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>36.59</td>\n",
       "      <td>1.97</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          baseflow_index  sand_perc  silt_perc  clay_perc  organic_perc  \\\n",
       "gauge_id                                                                  \n",
       "53018               0.57      31.51      32.07      36.42          0.74   \n",
       "33034               0.77      56.74      23.23      20.03          1.31   \n",
       "55018               0.56      36.89      36.01      27.09          0.47   \n",
       "\n",
       "          gauge_elev     area  dpsbar  elev_mean  elev_min  elev_10  elev_50  \\\n",
       "gauge_id                                                                       \n",
       "53018           18.0  1569.35    48.9      109.0      20.3     53.3    104.0   \n",
       "33034            7.2   707.75    16.3       42.0       8.1     25.3     41.7   \n",
       "55018           55.4   143.82    68.0      138.0      57.2     77.0    142.3   \n",
       "\n",
       "          elev_90  elev_max  dwood_perc  ewood_perc  grass_perc  shrub_perc  \\\n",
       "gauge_id                                                                      \n",
       "53018       172.1     304.6        6.38        0.65       50.46        0.04   \n",
       "33034        60.0      94.6        8.49        7.07       13.07        0.33   \n",
       "55018       199.2     252.4        5.40        0.12       56.11        0.00   \n",
       "\n",
       "          crop_perc  urban_perc  inwater_perc  bares_perc  surfacewater_abs  \\\n",
       "gauge_id                                                                      \n",
       "53018         35.01        6.91          0.19        0.48             0.034   \n",
       "33034         65.76        5.14          0.30        0.00             0.004   \n",
       "55018         36.59        1.97          0.04        0.03             0.000   \n",
       "\n",
       "          groundwater_abs  discharges  num_reservoir  reservoir_cap  \\\n",
       "gauge_id                                                              \n",
       "53018               0.051       0.066              0              0   \n",
       "33034               0.046       0.020              0              0   \n",
       "55018               0.000       0.007              0              0   \n",
       "\n",
       "          chalk_stream_flag  \n",
       "gauge_id                     \n",
       "53018                 False  \n",
       "33034                  True  \n",
       "55018                 False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if example_input:\n",
    "\n",
    "    # Example 2D dataset with date, dynamic, and label columns\n",
    "    timeseries = {\n",
    "        date_field: pd.date_range(start='2023-01-01', periods=20+15+10, freq='D'),\n",
    "        'catchmentID': ['10002'] * 20 + ['10003'] * 15 + ['10004'] * 10,\n",
    "        f\"{date_field}_group\": ['00'] * 15 + ['01'] * 5 + ['0'] * 8 + ['01'] * 7 + ['00'] * 8 + ['01'] * 2,\n",
    "        'precipitation': np.random.rand(20+15+10)*100,\n",
    "        'temperature': np.random.rand(20+15+10)*100,\n",
    "        'humidity': np.random.rand(20+15+10)*100,\n",
    "        'time_ref': range(20+15+10),\n",
    "        label_field: np.arange(20+15+10)*20\n",
    "    }\n",
    "\n",
    "    # Static variables\n",
    "    attributes = {\n",
    "        'catchmentID': ['10002', '10003', '10004'],\n",
    "        'silt_perc': [10, 20, 30],\n",
    "        'clay_perc': [0.5, 0.6, 0.7]\n",
    "    }\n",
    "\n",
    "    # Create DataFrames\n",
    "    attributes_df = pd.DataFrame(attributes).set_index('catchmentID')\n",
    "    timeseries_df = pd.DataFrame(timeseries)\n",
    "    \n",
    "    # Variables to be windowed\n",
    "    X_vars_names = [\n",
    "        \"precipitation\",\n",
    "        \"temperature\",\n",
    "        \"humidity\",\n",
    "        \"time_ref\"\n",
    "    ]\n",
    "    \n",
    "    # Define which columns need to be scaled with specific scaler\n",
    "    X_minmax_columns_names_list = ['time_ref']  # Column names for MinMaxScaler\n",
    "    X_standard_columns_names_list = ['precipitation', 'temperature', 'humidity']  # Column names for StandardScaler\n",
    "    \n",
    "    # Get sets to perform checks and warnings\n",
    "    X_minmax_columns_names_set = set(X_minmax_columns_names_list)\n",
    "    X_standard_columns_names_set = set(X_standard_columns_names_list)\n",
    "    \n",
    "    # Define window size\n",
    "    sequence_length = 2\n",
    "    \n",
    "    # Define label field for model\n",
    "    label_field_model = label_field\n",
    "    \n",
    "   \n",
    "    # Time range's first year\n",
    "    start_year = 2020\n",
    "\n",
    "    # NO `example_input` parameters\n",
    "    bfi = cs = csf = \"N\"\n",
    "    \n",
    "else:\n",
    "\n",
    "    # __________\n",
    "    # Timeseries\n",
    "    \n",
    "    timeseries_df = load_timeseries_df(\n",
    "        camels_gb_data_timeseries_aggr_dir,\n",
    "        \"timeseries_postFEa.csv\",\n",
    "        date_field\n",
    "    )\n",
    "\n",
    "    display(timeseries_df.head(3))\n",
    "    \n",
    "    \n",
    "    # __________\n",
    "    # Attributes  \n",
    "\n",
    "    attributes_df = load_attributes_df(\n",
    "        camels_gb_data_attributes_aggr_dir,\n",
    "        \"fundamental_postFEa.csv\",\n",
    "        attributes_index\n",
    "    )\n",
    "\n",
    "    display(attributes_df.head(3))\n",
    "    \n",
    "    \n",
    "    # _____________\n",
    "    # Aux variables\n",
    "    \n",
    "    # Variables to be windowed\n",
    "    X_vars_names = [\n",
    "        \"precipitation\",\n",
    "        \"temperature\",\n",
    "        \"humidity\",\n",
    "        \"shortwave_rad\",\n",
    "        \"longwave_rad\",\n",
    "        \"windspeed\",\n",
    "        \"sin_year\",\n",
    "        \"cos_year\",\n",
    "        \"time_ref\"\n",
    "    ]\n",
    "    \n",
    "    # Column names for MinMaxScaler\n",
    "    X_minmax_columns_names_list = [     \n",
    "        'time_ref'\n",
    "    ] \n",
    "    \n",
    "    # Column names for StandardScaler\n",
    "    X_standard_columns_names_list = [\n",
    "        \"precipitation\",\n",
    "        \"temperature\",\n",
    "        \"humidity\",\n",
    "        \"shortwave_rad\",\n",
    "        \"longwave_rad\",\n",
    "        \"windspeed\"\n",
    "    ]  \n",
    "    \n",
    "    # Get sets to perform checks and warnings\n",
    "    X_minmax_columns_names_set = set(X_minmax_columns_names_list)\n",
    "    X_standard_columns_names_set = set(X_standard_columns_names_list)\n",
    "    \n",
    "    # Define window size\n",
    "    sequence_length = 30 # 30 15 10 5\n",
    "    \n",
    "    # Define transformed label\n",
    "    label_transformation = \"log1p\"\n",
    "    \n",
    "    # Define label field for model\n",
    "    label_transformed_field = f\"{label_transformation}_{label_field}\"\n",
    "\n",
    "    #___________________________\n",
    "    # \"Moving fields\" management\n",
    "    \n",
    "    # Which one do you want??\n",
    "    label_field_model = label_transformed_field # label_transformed_field / label_field\n",
    "    \n",
    "    # Time range's first year\n",
    "    start_year = 1985\n",
    "\n",
    "    # Do you want to include `baseflow_index`?\n",
    "    bfi = \"N\" # \"Y\" / \"N\"\n",
    "\n",
    "    # Do you want to include chalk stream catchments?\n",
    "    cs = \"N\" # \"Y\" / \"N\"\n",
    "\n",
    "    # Do you want to include `chalk_stream_flag`?\n",
    "    csf = \"N\" # \"Y\" / \"N\"\n",
    "\n",
    "    # Overwrite, as `chalk_stream_flag` is useless without chalk stream catchments..\n",
    "    #..as it would be a column of `False`, but cs == \"Y\" and csf = \"N\" is allowed\n",
    "    if cs == \"N\":\n",
    "        csf = \"N\"\n",
    "    #___________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59679fc5-6994-40d3-ad24-669613e04f6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Checks & Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36b2af89-bd19-49b2-9551-65226980363f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that will NOT be scaled:\n",
      "['sin_year', 'cos_year']\n"
     ]
    }
   ],
   "source": [
    "assert X_minmax_columns_names_set.isdisjoint(X_standard_columns_names_set), (\n",
    "    \"There are repetition between columns to be scaled with MinMax scaler and those to be scaled with Standard scaler\"\n",
    ")\n",
    "\n",
    "# Warnings\n",
    "X_no_scaled_columns_names_list = list(set(X_vars_names) - X_minmax_columns_names_set - X_standard_columns_names_set)\n",
    "if len(X_no_scaled_columns_names_list) != 0:\n",
    "    print(\"Columns that will NOT be scaled:\")\n",
    "    pprint(X_no_scaled_columns_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7022d528-97b7-4d1b-8799-dcba154bd779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if example_input:\n",
    "    display(attributes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dc59d14-b253-4529-aa79-bb7c66e650f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if example_input:\n",
    "    display(timeseries_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eaeab7-e629-487f-a18e-87f4e68ea007",
   "metadata": {},
   "source": [
    "### General parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c1eca1d-d225-4d4c-9093-34ddb272521f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = 0.7\n",
    "test_size = 1 - train_size\n",
    "min_required_obs = sequence_length / test_size\n",
    "X_n_vars=len(X_vars_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4faf59-7b6d-481d-962e-00cf5111a5e3",
   "metadata": {},
   "source": [
    "# Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d0c4e-a333-434b-9943-78e5bce92d4b",
   "metadata": {},
   "source": [
    "## `baseflow_index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44670c35-f131-4887-bc27-b342fab9c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_col_pre = attributes_df.shape[1]\n",
    "\n",
    "if bfi == \"N\":\n",
    "    (\n",
    "        attributes_df\n",
    "            .drop(\n",
    "                columns=['baseflow_index'],\n",
    "                inplace=True\n",
    "            )\n",
    "    )\n",
    "\n",
    "    assert n_col_pre - attributes_df.shape[1] == 1, ('Inconsistency after trying to remove `baseflow_index`')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e273057-6a30-4a03-b282-b2c7b10fdfd2",
   "metadata": {},
   "source": [
    "## Chalk streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2765a658-f8e9-441d-904c-c93f31cadd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row_pre, n_col_pre = attributes_df.shape\n",
    "\n",
    "# If chalk stream catchments need to be removed\n",
    "if cs == \"N\":\n",
    "    \n",
    "    # Define chalk stream catchments list (useful for time series)\n",
    "    chalk_streams_list = attributes_df[attributes_df['chalk_stream_flag'] == True].index.to_list()\n",
    "    \n",
    "    # Remove chalk stream catchments\n",
    "    attributes_df = attributes_df[~attributes_df.index.isin(chalk_streams_list)]\n",
    "\n",
    "    assert n_row_pre > attributes_df.shape[0], ('Inconsistency after trying to remove chalk stream catchments')\n",
    "\n",
    "# If either \"N\" conditions on chalk streams are met\n",
    "if cs == \"N\" or csf == \"N\":\n",
    "\n",
    "    # Remove `chalk_stream_flag`\n",
    "    attributes_df.drop(columns=['chalk_stream_flag'], inplace=True)\n",
    "\n",
    "    assert n_col_pre -  attributes_df.shape[1], ('Inconsistency after trying to remove `chalk_stream_flag`')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a60dfc6-00c2-4f03-91e3-31b167b39825",
   "metadata": {},
   "source": [
    "## Scaler details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1217d703-0e60-4a8b-bba6-42c5f1ebacea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving all columns from attributes as list\n",
    "X_static_vars_names = attributes_df.columns.to_list()\n",
    "X_static_standard_columns_names_list = X_static_vars_names.copy()\n",
    "X_static_no_scaled_columns_names_list = []\n",
    "\n",
    "# Removing 'baseflow_index' as spans [0, 1] by construction\n",
    "if bfi == \"Y\":\n",
    "    X_static_standard_columns_names_list.remove('baseflow_index')\n",
    "    X_static_no_scaled_columns_names_list.append('baseflow_index')\n",
    "\n",
    "# Removing 'chalk_stream_flag' as is already Boolean\n",
    "if csf == \"Y\":\n",
    "    X_static_standard_columns_names_list.remove('chalk_stream_flag')\n",
    "    X_static_no_scaled_columns_names_list.append('chalk_stream_flag')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065f570c-1629-4105-8b4b-5eae0da84e58",
   "metadata": {},
   "source": [
    "# Timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b3cfcc-4b68-46bf-b7ca-5051672b3c79",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Filter for time range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e68bb71b-e2f0-4791-9673-7331c27dad6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "timeseries_df = (\n",
    "    timeseries_df[\n",
    "        pd.to_datetime(timeseries_df[date_field]) >= datetime(start_year, 9, 30) # Because most recent observation: 30/09/2015\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c81703-597b-4da1-9fbb-e5f10e920c4a",
   "metadata": {},
   "source": [
    "## Chalk streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "118a2600-2b79-4eb0-9242-adbc5195069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_row_pre = timeseries_df.shape[0]\n",
    "\n",
    "if cs == \"N\":\n",
    "    timeseries_df = (\n",
    "        timeseries_df[\n",
    "            ~timeseries_df['catchmentID'].isin(chalk_streams_list)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    assert n_row_pre > timeseries_df.shape[0], (\"Inconsistency after trying to remove chalk stream catchments' TS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a82673-e334-4442-b023-c068604f4a53",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sensor data windowing and registry (function definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "044241fc-6dc2-41eb-8d95-c44a656ce9ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_timeseries_for_sensor(sensor_data,\n",
    "                                 sensor_id,\n",
    "                                 group,\n",
    "                                 sequence_length=15,\n",
    "                                 sampling_rate=1,\n",
    "                                 sequence_stride=1\n",
    "):\n",
    "    \n",
    "    # ____________________________________\n",
    "    # Split train/test (without shuffling)\n",
    "    \n",
    "    # Train and test set\n",
    "    X_train, X_test, y_train, y_test = (\n",
    "        train_test_split(\n",
    "            sensor_data[X_vars_names].values,\n",
    "            sensor_data[label_field_model].values,\n",
    "            train_size=train_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Timestamps\n",
    "    timestamps_train, timestamps_test = (\n",
    "        train_test_split(\n",
    "            sensor_data[date_field].values,\n",
    "            train_size=train_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # _________\n",
    "    # Windowing\n",
    "    for X, y, timestamps, curr_set in zip(\n",
    "        [X_train, X_test],\n",
    "        [y_train, y_test],\n",
    "        [timestamps_train, timestamps_test],\n",
    "        ['train', 'test']\n",
    "    ):\n",
    "    \n",
    "        # ______________________________\n",
    "        # Windowing and numpy conversion\n",
    "\n",
    "        # Launch `keras.utils.timeseries_dataset_from_array` to generate the windows\n",
    "        dataset = timeseries_dataset_from_array(\n",
    "            X,\n",
    "            targets=y[(sequence_length-1):],\n",
    "            sequence_length=sequence_length,\n",
    "            sampling_rate=sampling_rate,\n",
    "            sequence_stride=sequence_stride,\n",
    "            batch_size=1 # Must be ️1, see related comment🅰️\n",
    "        )\n",
    "\n",
    "        # Collect the data from batches in the dataset\n",
    "        sequence_list=[]\n",
    "        target_list = []\n",
    "        for curr_sequence, curr_target in dataset:  # See previous comment 🅰️\n",
    "            sequence_list.append(curr_sequence)\n",
    "            target_list.append(curr_target)\n",
    "\n",
    "        # Convert into a 3D numpy array\n",
    "        sequence = np.array(sequence_list).reshape(-1, sequence_length, X_n_vars)\n",
    "        target = np.array(target_list).reshape(-1)\n",
    "\n",
    "\n",
    "        # ____________________\n",
    "        # Observation registry\n",
    "\n",
    "        # Calculate timestamps tuple with start and end dates for each observation\n",
    "        timestamps = [\n",
    "            (timestamps[i], timestamps[i + sequence_length - 1])\n",
    "            for i in range(len(timestamps) - sequence_length + 1)\n",
    "        ]\n",
    "\n",
    "        # Generate data frame to store registry\n",
    "        sensor_registry_df = pd.DataFrame(timestamps, columns=['start_date', 'end_date'])\n",
    "        sensor_registry_df.insert(0, 'group', group)\n",
    "        sensor_registry_df.insert(0, 'catchmentID', sensor_id)\n",
    "\n",
    "\n",
    "        # ______\n",
    "        # Checks\n",
    "        n_sequence = sequence.shape[0]\n",
    "        n_target = target.shape[0]\n",
    "        n_timestamps = len(timestamps)\n",
    "        assert n_sequence == n_target == n_timestamps, \"Sensor' sequence, target, and timestamp for data frame must have the same length for first dimension\"\n",
    "    \n",
    "        # ______________\n",
    "        # Set allocation\n",
    "        if curr_set == 'train':\n",
    "            sequence_train = sequence\n",
    "            target_train = target\n",
    "            sensor_registry_train_df = sensor_registry_df\n",
    "        \n",
    "        elif curr_set == 'test':\n",
    "            sequence_test = sequence\n",
    "            target_test = target\n",
    "            sensor_registry_test_df = sensor_registry_df\n",
    "    \n",
    "    return (sequence_train, target_train), (sequence_test, target_test), (sensor_registry_train_df, sensor_registry_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aec6eca-5c52-4b53-a1cf-7d85762e93ae",
   "metadata": {},
   "source": [
    "## Sensor data aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b250f9c-4480-4a13-9b4c-e967790b9b09",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing catchmentID-groups:   0%|          | 0/15522 [00:00<?, ?it/s]2025-05-09 13:13:29.392687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:29.395449: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:29.397651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:29.399781: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:29.432678: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:29.434981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:29.437096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:29.439140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:29.441188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:29.443191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:29.445277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:29.447294: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:29.452861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-09 13:13:30.879422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.881933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.883698: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.885848: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.888007: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.889833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.891855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.893892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.895933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.897767: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.899378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.901419: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.946917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.948898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.950610: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.952725: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.956069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.957945: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.959586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.961645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.963722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.966366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 122 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "2025-05-09 13:13:30.969750: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.971397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13564 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n",
      "2025-05-09 13:13:30.971998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.973995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 13564 MB memory:  -> device: 2, name: Tesla T4, pci bus id: 0000:00:06.0, compute capability: 7.5\n",
      "2025-05-09 13:13:30.975402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-09 13:13:30.977401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 13564 MB memory:  -> device: 3, name: Tesla T4, pci bus id: 0000:00:07.0, compute capability: 7.5\n",
      "2025-05-09 13:13:31.049677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:735] failed to allocate 122.75M (128712704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "Processing catchmentID-groups: 100%|██████████| 15522/15522 [3:34:01<00:00,  1.21it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Initialize aggregate variables\n",
    "X_train,  X_test = np.empty((0, sequence_length, X_n_vars)), np.empty((0, sequence_length, X_n_vars))\n",
    "y_train, y_test = np.array([]), np.array([])\n",
    "\n",
    "# Initialize data frames for registry\n",
    "registry_train_df = pd.DataFrame()\n",
    "registry_test_df = pd.DataFrame()\n",
    "\n",
    "# Set verbosity\n",
    "verbose=example_input\n",
    "\n",
    "# Loop on sensor data to aggregate the data \n",
    "for (curr_sensor_id, curr_date_group), curr_sensor_df in tqdm(\n",
    "        timeseries_df.groupby(['catchmentID', f\"{date_field}_group\"]),\n",
    "        desc=\"Processing catchmentID-groups\"\n",
    "):\n",
    "\n",
    "    # Define current \"sensor-group\"\n",
    "    curr_sensor_group = f\"{curr_sensor_id}-{curr_date_group}\"\n",
    "    \n",
    "    # Calculate number of observations\n",
    "    n_curr_sensor_obs = curr_sensor_df.shape[0]\n",
    "    \n",
    "    # Check on the minimum of observation required\n",
    "    if n_curr_sensor_obs >= min_required_obs:\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{curr_sensor_group}... \", end=\"\")\n",
    "    \n",
    "        # Function call\n",
    "        (curr_X_train, curr_y_train), (curr_X_test, curr_y_test), (curr_registry_train_df, curr_registry_test_df)  = (\n",
    "            create_timeseries_for_sensor(\n",
    "                curr_sensor_df,\n",
    "                curr_sensor_id,\n",
    "                curr_date_group,\n",
    "                sequence_length=sequence_length)\n",
    "        )\n",
    "\n",
    "        # Aggregate Train\n",
    "        X_train = np.concatenate((X_train, curr_X_train), axis=0)\n",
    "        y_train = np.concatenate((y_train, curr_y_train), axis=0)\n",
    "\n",
    "        # Aggregate Test\n",
    "        X_test = np.concatenate((X_test, curr_X_test), axis=0)\n",
    "        y_test = np.concatenate((y_test, curr_y_test), axis=0)\n",
    "\n",
    "        # Aggregate registry\n",
    "        registry_train_df = pd.concat([registry_train_df, curr_registry_train_df], ignore_index=True)\n",
    "        registry_test_df = pd.concat([registry_test_df, curr_registry_test_df], ignore_index=True)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"OK!\\t\", end=\"\")\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{curr_sensor_group}: too short timeseries ({n_curr_sensor_obs} obs.)\\t\", end=\"\")\n",
    "\n",
    "\n",
    "# Checks\n",
    "n_sequence = X_train.shape[0]\n",
    "n_target = y_train.shape[0]\n",
    "n_registry = registry_train_df.shape[0]\n",
    "assert n_sequence == n_target == n_registry, \"Train sequence, target must, and registry data frame must have the same length for first dimension\"\n",
    "\n",
    "n_sequence = X_test.shape[0]\n",
    "n_target = y_test.shape[0]\n",
    "n_registry = registry_test_df.shape[0]\n",
    "assert n_sequence == n_target == n_registry, \"Tests sequence, target must, and registry data frame must have the same length for first dimension\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e3260e-e29b-4d6f-8ec0-c2bfd533fb6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train dimensions: (2403628, 30, 9)\n"
     ]
    }
   ],
   "source": [
    "if example_input:\n",
    "    print(X_train)\n",
    "else:\n",
    "    print(f\"X train dimensions: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca478581-20dd-469b-a219-c75a2756b22b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train dimensions: (2403628,)\n"
     ]
    }
   ],
   "source": [
    "if example_input:\n",
    "    print(y_train)\n",
    "else:\n",
    "    print(f\"y train dimensions: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9ded03-aec9-4fe8-b24b-24185746c608",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X test dimensions: (940116, 30, 9)\n"
     ]
    }
   ],
   "source": [
    "if example_input:\n",
    "    print(X_test)\n",
    "else:\n",
    "    print(f\"X test dimensions: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519ff5bb-4d69-420c-af31-c0249eee08eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y test dimensions: (940116,)\n"
     ]
    }
   ],
   "source": [
    "if example_input:\n",
    "    print(y_test)\n",
    "else:\n",
    "    print(f\"y test dimensions: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b1566-fa3b-4a6b-800c-b2048e9ed3c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `X_train` and `X_test` scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52187148-94f9-4645-b546-4ea763912244",
   "metadata": {},
   "source": [
    "### Columns management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e5c465-7dc3-4594-a664-420451fb41c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the indices of the columns\n",
    "X_minmax_columns_index = [X_vars_names.index(col) for col in X_minmax_columns_names_list]\n",
    "X_standard_columns_index = [X_vars_names.index(col) for col in X_standard_columns_names_list]\n",
    "X_no_scaled_columns_index = [X_vars_names.index(col) for col in X_no_scaled_columns_names_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c2436f-fd8b-41d5-acd9-44607336bef6",
   "metadata": {},
   "source": [
    "### `multi_scaling` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacbfa97-4ec4-46e9-95a7-806a1251836f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multi_scaling(\n",
    "    X,\n",
    "    preprocessor\n",
    "):\n",
    " \n",
    "    # Store original dimensions\n",
    "    n_samples, n_timesteps, n_features = X.shape\n",
    "    print(f\"Input with dimensions:\\t\\t\\t{X.shape}\")\n",
    "\n",
    "    # Reshape to 2D\n",
    "    X_2D = X.reshape(-1, n_features)\n",
    "    print(f\"Reshaped to 2D with dimensions:\\t\\t{X_2D.shape}\")\n",
    "\n",
    "    # Scale the 2D\n",
    "    X_scaled_2D = preprocessor.transform(X_2D)\n",
    "    print(f\"After transformation dimensions:\\t{X_scaled_2D.shape}\")\n",
    "\n",
    "    # Reshape back to 3D\n",
    "    X_scaled_3D = X_scaled_2D.reshape(n_samples, n_timesteps, n_features)\n",
    "    print(f\"Reshaped to 3D with dimensions:\\t\\t{X_scaled_3D.shape}\")\n",
    "\n",
    "    return X_scaled_3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba83b32a-5e89-440b-8036-dd3e4a8d2df7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e29969-ab6b-4a18-881b-4f8f820125f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mold created with dimensions:\t\t\t\t(72108840, 9)\n",
      "Mold reduced by duplicated colums with dimensions:\t(2567849, 9)\n",
      "\n",
      "Train\n",
      "Input with dimensions:\t\t\t(2403628, 30, 9)\n",
      "Reshaped to 2D with dimensions:\t\t(72108840, 9)\n",
      "After transformation dimensions:\t(72108840, 9)\n",
      "Reshaped to 3D with dimensions:\t\t(2403628, 30, 9)\n",
      "\n",
      "Test\n",
      "Input with dimensions:\t\t\t(940116, 30, 9)\n",
      "Reshaped to 2D with dimensions:\t\t(28203480, 9)\n",
      "After transformation dimensions:\t(28203480, 9)\n",
      "Reshaped to 3D with dimensions:\t\t(940116, 30, 9)\n"
     ]
    }
   ],
   "source": [
    "# _______________________\n",
    "# Define the preprocessor\n",
    "\n",
    "# Reshape train set (used as mold data) to 2D\n",
    "X_mold = X_train.reshape(-1, X_n_vars)\n",
    "print(f\"Mold created with dimensions:\\t\\t\\t\\t{X_mold.shape}\")\n",
    "\n",
    "# Remove repeated rows which comes with the windowing (avoiding to distort the distribution)\n",
    "X_mold = np.unique(X_mold, axis=0)\n",
    "print(f\"Mold reduced by duplicated rows with dimensions:\\t{X_mold.shape}\", end=\"\\n\")\n",
    "\n",
    "# Define the ColumnTransformer\n",
    "X_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('minmax', MinMaxScaler(), X_minmax_columns_index),\n",
    "        ('standard', StandardScaler(), X_standard_columns_index),\n",
    "        ('no_scale', 'passthrough', X_no_scaled_columns_index)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Numpy new columns names order\n",
    "X_cols_names = X_minmax_columns_names_list + X_standard_columns_names_list + X_no_scaled_columns_names_list\n",
    "\n",
    "# Fit the preprocessor\n",
    "X_preprocessor.fit(X_mold)\n",
    "\n",
    "print(\"\\nTrain\")\n",
    "\n",
    "# _____________\n",
    "# Apply scaling\n",
    "\n",
    "# Train\n",
    "X_train_scaled = multi_scaling(\n",
    "    X_train,\n",
    "    X_preprocessor\n",
    ")\n",
    "\n",
    "print(\"\\nTest\")\n",
    "\n",
    "# Test\n",
    "X_test_scaled = multi_scaling(\n",
    "    X_test,\n",
    "    X_preprocessor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc95ae37-5b09-4867-a1e0-158d6a7a1a3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled X train dimensions:(2403628, 30, 9)\n"
     ]
    }
   ],
   "source": [
    "if example_input:\n",
    "    print(X_train_scaled)\n",
    "else:\n",
    "    print(f\"Scaled X train dimensions:{X_train_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7357784-f2b7-4462-b2c2-c7c055938341",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled X test dimensions: (940116, 30, 9)\n"
     ]
    }
   ],
   "source": [
    "if example_input:\n",
    "    print(X_test_scaled)\n",
    "else:\n",
    "    print(f\"Scaled X test dimensions: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c526579b-9948-452d-8ea3-a43b5c62f342",
   "metadata": {},
   "source": [
    "## Train set shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b355813-a32a-4beb-b860-383124fa2e78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the number of samples\n",
    "n_samples = X_train_scaled.shape[0]\n",
    "\n",
    "# Set the seed\n",
    "np.random.seed(82)\n",
    "\n",
    "# Generate a random permutation of indices\n",
    "shuffle_indices = np.random.permutation(n_samples)\n",
    "\n",
    "# Shuffle the X training set\n",
    "X_train_scaled_shuffled = X_train_scaled[shuffle_indices]\n",
    "\n",
    "# Shuffle the y training set\n",
    "y_train_shuffled = y_train[shuffle_indices]\n",
    "\n",
    "# Shuffle the dataframe using the same indices (🚩 without resetting the index!! 🚩)\n",
    "registry_train_df_shuffled = registry_train_df.iloc[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c422d10a-3909-42d5-80d3-3b8f5aed5e94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled & scaled X train dimensions: (2403628, 30, 9)\n"
     ]
    }
   ],
   "source": [
    "if example_input:\n",
    "    print(X_train_scaled_shuffled)\n",
    "else:\n",
    "    print(f\"Shuffled & scaled X train dimensions: {X_train_scaled_shuffled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d2593-c132-4f51-8728-7ed9dadaa986",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled & scaled y train dimensions: (2403628,)\n"
     ]
    }
   ],
   "source": [
    "if example_input:\n",
    "    print(y_train_shuffled)\n",
    "else:\n",
    "    print(f\"Shuffled & scaled y train dimensions: {y_train_shuffled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a3324-e0b9-4888-89d1-2fa883ad5a5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled registry df dimensions: (2403628, 4)\n"
     ]
    }
   ],
   "source": [
    "if example_input:\n",
    "    display(registry_train_df_shuffled)\n",
    "else:\n",
    "    print(f\"Shuffled registry df dimensions: {registry_train_df_shuffled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6fece1-ad98-4673-92e1-ff034a764867",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Attributes (*static variables*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a89b31-a9a8-4c8c-a475-782eeabb2000",
   "metadata": {},
   "source": [
    "## Attributes merge via registry (function definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da268b0e-3d15-4e40-abf6-7f47e8bd5293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_attributes(registry):\n",
    "    \n",
    "    # Define columns not to be put in the output (any columns from the registry which is not 'catchmentID')\n",
    "    registry_redundant_columns_set = set(registry.columns)\n",
    "    registry_redundant_columns_set.remove('catchmentID')\n",
    "\n",
    "    # Perform the left join\n",
    "    merged_df = registry.merge(\n",
    "        attributes_df,\n",
    "        left_on='catchmentID',\n",
    "        right_index=True,\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Drop redundant columns\n",
    "    merged_df = merged_df.drop(columns=registry_redundant_columns_set)\n",
    "\n",
    "    # Set the index to 'catchmentID'\n",
    "    merged_df = merged_df.set_index('catchmentID')\n",
    "    \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e6834f-31de-4ab3-9ca0-9ad2e994870c",
   "metadata": {},
   "source": [
    "## Train and test set merge & scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7928fd21-0369-4c31-a4ec-ae066f78a9ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the indices of the columns for standard scaler\n",
    "X_static_standard_columns_index = [X_static_vars_names.index(col) for col in X_static_standard_columns_names_list]\n",
    "\n",
    "# ________________________\n",
    "# Define transformers list\n",
    "transformers=[\n",
    "    ('standard', StandardScaler(), X_static_standard_columns_index)\n",
    "]\n",
    "\n",
    "if csf == \"Y\" or bfi == \"Y\":\n",
    "\n",
    "    # Get the indices of the columns with no scaling\n",
    "    X_static_no_scaled_columns_index = [X_static_vars_names.index(col) for col in X_static_no_scaled_columns_names_list]\n",
    "    transformers.append(\n",
    "         ('no_scale', 'passthrough', X_static_no_scaled_columns_index)\n",
    "    )\n",
    "\n",
    "\n",
    "X_static_cols_names = X_static_standard_columns_names_list + X_static_no_scaled_columns_names_list\n",
    "\n",
    "# _____\n",
    "# Train\n",
    "\n",
    "# Merge\n",
    "X_train_static_df = merge_attributes(registry_train_df_shuffled)\n",
    "\n",
    "# Define the ColumnTransformer\n",
    "X_static_preprocessor = ColumnTransformer(\n",
    "    transformers=transformers\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "X_train_static_scaled_df = (\n",
    "    pd.DataFrame(\n",
    "        X_static_preprocessor.fit_transform(X_train_static_df),\n",
    "        columns=X_train_static_df.columns\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# ____\n",
    "# Test\n",
    "\n",
    "# Merge\n",
    "X_test_static_df = merge_attributes(registry_test_df)\n",
    "\n",
    "# Transform only\n",
    "X_test_static_scaled_df = (\n",
    "    pd.DataFrame(\n",
    "        X_static_preprocessor.transform(X_test_static_df),\n",
    "        columns=X_test_static_df.columns\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78abbb67-87c8-4e8c-92ea-102deb1f9f2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled registry df AFTER merging dimensions: (2403628, 27)\n"
     ]
    }
   ],
   "source": [
    "if example_input:\n",
    "    display(X_train_static_scaled_df)\n",
    "else:\n",
    "    print(f\"Shuffled registry df AFTER merging dimensions: {X_train_static_scaled_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a327c2c8-5750-4fb1-b107-a46198d085ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled registry df AFTER merging dimensions: (940116, 27)\n"
     ]
    }
   ],
   "source": [
    "if example_input:\n",
    "    display(X_train_static_scaled_df)\n",
    "else:\n",
    "    print(f\"Shuffled registry df AFTER merging dimensions: {X_test_static_scaled_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fd124e-8423-49b7-9962-58d3f28c3358",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd822433-2f12-4d9a-a3d7-0ca3a5249169",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reset train registry index 🚩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198b68ce-a459-4f5b-85da-8f91bd610950",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>catchmentID</th>\n",
       "      <th>group</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54096</td>\n",
       "      <td>01</td>\n",
       "      <td>2001-12-13</td>\n",
       "      <td>2002-01-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40004</td>\n",
       "      <td>31</td>\n",
       "      <td>2010-09-02</td>\n",
       "      <td>2010-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33023</td>\n",
       "      <td>44</td>\n",
       "      <td>2004-09-01</td>\n",
       "      <td>2004-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39061</td>\n",
       "      <td>05</td>\n",
       "      <td>1987-12-28</td>\n",
       "      <td>1988-01-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76005</td>\n",
       "      <td>37</td>\n",
       "      <td>2004-03-03</td>\n",
       "      <td>2004-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2403623</th>\n",
       "      <td>42008</td>\n",
       "      <td>02</td>\n",
       "      <td>1987-08-24</td>\n",
       "      <td>1987-09-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2403624</th>\n",
       "      <td>44006</td>\n",
       "      <td>00</td>\n",
       "      <td>1986-09-22</td>\n",
       "      <td>1986-10-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2403625</th>\n",
       "      <td>45005</td>\n",
       "      <td>16</td>\n",
       "      <td>1985-12-05</td>\n",
       "      <td>1986-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2403626</th>\n",
       "      <td>76014</td>\n",
       "      <td>27</td>\n",
       "      <td>1999-08-20</td>\n",
       "      <td>1999-09-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2403627</th>\n",
       "      <td>28072</td>\n",
       "      <td>06</td>\n",
       "      <td>2011-07-04</td>\n",
       "      <td>2011-08-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2403628 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        catchmentID group  start_date    end_date\n",
       "0             54096    01  2001-12-13  2002-01-11\n",
       "1             40004    31  2010-09-02  2010-10-01\n",
       "2             33023    44  2004-09-01  2004-09-30\n",
       "3             39061    05  1987-12-28  1988-01-26\n",
       "4             76005    37  2004-03-03  2004-04-01\n",
       "...             ...   ...         ...         ...\n",
       "2403623       42008    02  1987-08-24  1987-09-22\n",
       "2403624       44006    00  1986-09-22  1986-10-21\n",
       "2403625       45005    16  1985-12-05  1986-01-03\n",
       "2403626       76014    27  1999-08-20  1999-09-18\n",
       "2403627       28072    06  2011-07-04  2011-08-02\n",
       "\n",
       "[2403628 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train registry\n",
    "(\n",
    "    registry_train_df_shuffled\n",
    "        .reset_index(\n",
    "            inplace=True,\n",
    "            drop=True\n",
    "        )\n",
    ")\n",
    "\n",
    "display(registry_train_df_shuffled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b573cf86-1b02-43c2-a553-83f477703925",
   "metadata": {},
   "source": [
    "## Create the dictionary to collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cb665e-e836-41d4-b48d-8e31f1733c7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not example_input:\n",
    "    model_feed = {\n",
    "        'bfi': bfi,\n",
    "        'cs': cs,\n",
    "        'csf': csf,  \n",
    "        'train_size': train_size,\n",
    "        'test_size': test_size,\n",
    "        'min_required_obs': min_required_obs,\n",
    "        'X_minmax_columns_names_list': X_minmax_columns_names_list,\n",
    "        'X_standard_columns_names_list': X_standard_columns_names_list,\n",
    "        'X_no_scaled_columns_names_list': X_no_scaled_columns_names_list,\n",
    "        'X_static_standard_columns_names_list': X_static_standard_columns_names_list,\n",
    "        'X_static_no_scaled_columns_names_list': X_static_no_scaled_columns_names_list,\n",
    "        \"X_train\": X_train_scaled_shuffled,\n",
    "        \"y_train\": y_train_shuffled,\n",
    "        \"X_test\": X_test_scaled,\n",
    "        \"y_test\": y_test,\n",
    "        \"X_train_static_df\": X_train_static_scaled_df,\n",
    "        \"X_test_static_df\": X_test_static_scaled_df,\n",
    "        \"X_train_registry_df\": registry_train_df_shuffled,\n",
    "        \"X_test_registry_df\": registry_test_df,\n",
    "        \"X_preprocessor\": X_preprocessor,\n",
    "        \"X_static_preprocessor\": X_static_preprocessor,\n",
    "        \"X_cols_names\": X_cols_names,\n",
    "        \"X_static_cols_names\": X_static_cols_names,\n",
    "        \"label_field_model\": label_field_model\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55f212f-cd76-4d99-8ea1-fa290e503a50",
   "metadata": {},
   "source": [
    "## Save the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c96ed3-1dc2-487b-8925-ee5e1bf7e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store\n",
    "if not example_input:\n",
    "    with open(\n",
    "        os.path.join(\n",
    "            camels_gb_silver_dir,\n",
    "            f\"model_feed-w{sequence_length}-{label_field_model}-{start_year}-bfi_{bfi}-cs_{cs}_csf-{csf}.pkl\"\n",
    "        ),\n",
    "        'wb'\n",
    "    ) as f:\n",
    "        pickle.dump(model_feed, f)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "mlflow-geo-inequalipy-venn-missingno",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Mlflow-Geo-Inequalipy-Venn-Missingno (Local)",
   "language": "python",
   "name": "mlflow-geo-inequalipy-venn-missingno"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
